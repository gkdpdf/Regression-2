{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "316f9dfd-6d1b-40bf-b712-75dc586abf37",
   "metadata": {},
   "source": [
    "## 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e04f83a-c3ca-4549-9877-b3e4a8bf3120",
   "metadata": {},
   "source": [
    "R-squared (Coefficient of Determination) in Linear Regression:\n",
    "\n",
    "Definition:\n",
    "R-squared (R 2) is a statistical measure that represents the proportion of the variance in the dependent variable (target) that is explained by the independent variable(s) in a linear regression model. It is a measure of the goodness of fit of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b4d607-dc4c-4fff-824f-5a6037559ed7",
   "metadata": {},
   "source": [
    "Calculation:\n",
    "The formula for calculating \n",
    "\n",
    "R \n",
    "2\n",
    "\n",
    "  is as follows:\n",
    "\n",
    "\n",
    "=\n",
    "1\n",
    "−\n",
    "Sum of Squared Residuals (SSR)/Total Sum of Squares (SST)\n",
    "\n",
    "Where:\n",
    "\n",
    "SSR (Sum of Squared Residuals) is the sum of the squared differences between the observed values and the predicted values.\n",
    "SST (Total Sum of Squares) is the sum of the squared differences between the observed values and the mean of the observed values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1243b35-b306-423c-b74e-66b93e4c815e",
   "metadata": {},
   "source": [
    "Interpretation:\n",
    "\n",
    "R 2 ranges from 0 to 1.\n",
    "\n",
    "R 2 =0 indicates that the model does not explain any variability in the dependent variable.\n",
    "\n",
    "R 2 =1 indicates that the model perfectly explains the variability in the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e13b69-c754-4f58-bc09-68b56e614462",
   "metadata": {},
   "source": [
    "Interpretation Guidelines:\n",
    "\n",
    "A higher R 2 suggests a better fit of the model to the data.\n",
    "\n",
    "R 2 can be interpreted as the proportion of variance in the dependent variable that is \"captured\" by the independent variable(s).\n",
    "\n",
    "R 2\n",
    "does not indicate the correctness of the model specification or the causal relationship between variables. It only assesses the goodness of fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842796e2-3079-4418-b509-d1222622ad64",
   "metadata": {},
   "source": [
    "R 2  tends to increase as more predictors are added to the model, even if they are not truly adding explanatory power. Adjusted \n",
    "R 2 is often used to account for this by penalizing the inclusion of irrelevant predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8677fbaf-3d41-476e-ad47-6dab5a50f640",
   "metadata": {},
   "source": [
    "## 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3831a6a2-5aa5-4d9e-9ab8-c476d18f0a29",
   "metadata": {},
   "source": [
    "Adjusted R-squared is a modified version of the regular R-squared (R 2) in linear regression models. It takes into account the number of predictors (independent variables) in the model, addressing a limitation of R 2 that tends to increase with the addition of more predictors, regardless of their actual contribution to explaining the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d27f54-ae52-4ef3-9102-077ccfa6c1e9",
   "metadata": {},
   "source": [
    "Calculation:\n",
    "The formula for calculating Adjusted R-squared is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38515b39-a62e-4c21-8d15-fc2013920018",
   "metadata": {},
   "source": [
    "Adjusted R 2=1−(1−R 2)×(n−1)/ n−k−1\n",
    "Where:\n",
    "\n",
    "n is the number of observations.\n",
    "k is the number of predictors in the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3a05aa-8133-489f-9cf2-bb09d904b642",
   "metadata": {},
   "source": [
    "Interpretation:\n",
    "\n",
    "Higher values of Adjusted R 2\n",
    "indicate a better balance between model fit and complexity.\n",
    "Adjusted R 2\n",
    "is a more conservative measure of goodness of fit, reflecting the trade-off between model complexity and the amount of explained variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08304afb-f656-4a7f-a1e7-ab34b3b3ea8d",
   "metadata": {},
   "source": [
    "## 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51653586-7af0-48b1-a755-c1908fd7e837",
   "metadata": {},
   "source": [
    "\n",
    "Adjusted R-squared is more appropriate to use in situations where you are dealing with multiple linear regression models with varying numbers of predictors (independent variables). Here are some scenarios where the use of Adjusted R-squared is particularly relevant:\n",
    "\n",
    "Comparing Models with Different Numbers of Predictors:\n",
    "\n",
    "When you are comparing multiple linear regression models with different numbers of predictors, Adjusted R-squared is more appropriate. Regular R-squared may increase simply by adding more predictors, even if they do not contribute significantly to explaining the variance in the dependent variable.\n",
    "Avoiding Overfitting:\n",
    "\n",
    "Adjusted R-squared penalizes the inclusion of additional predictors that do not improve the model significantly. This makes it a useful metric for avoiding overfitting, where a model becomes too complex and fits the noise in the data rather than the underlying pattern.\n",
    "Model Selection:\n",
    "\n",
    "In the process of model selection, when deciding which variables to include in the model, Adjusted R-squared helps in evaluating the trade-off between explanatory power and model simplicity. It guides the selection of a model that strikes a balance between fit and complexity.\n",
    "Dealing with High-Dimensional Data:\n",
    "\n",
    "In situations where you have a high-dimensional dataset with a large number of potential predictors, Adjusted R-squared can be more informative. It guides you in selecting a subset of predictors that contribute meaningfully to the model's performance.\n",
    "Parsimony:\n",
    "\n",
    "Adjusted R-squared is a measure of goodness of fit that accounts for the number of predictors, promoting parsimony in model selection. It favors models that explain a substantial portion of the variance while using fewer predictors.\n",
    "Model Evaluation:\n",
    "\n",
    "Adjusted R-squared provides a more conservative estimate of the goodness of fit, reflecting the model's true explanatory power while adjusting for the complexity introduced by the number of predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a0488f-7fdc-4e4d-ac5e-aa91102616bc",
   "metadata": {},
   "source": [
    "## 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26622ee6-f8ab-4fcb-b6c4-7037c46c956d",
   "metadata": {},
   "source": [
    "\n",
    "In the context of regression analysis, RMSE (Root Mean Squared Error), MSE (Mean Squared Error), and MAE (Mean Absolute Error) are commonly used metrics to evaluate the performance of regression models by measuring the accuracy of their predictions against actual values.\n",
    "\n",
    "1. Mean Squared Error (MSE):\n",
    "MSE is a measure of the average squared difference between predicted and actual values. It is calculated by taking the average of the squared residuals (the differences between predicted and actual values):\n",
    "\n",
    "\n",
    "2. Root Mean Squared Error (RMSE):\n",
    "RMSE is the square root of the MSE and provides a measure of the average magnitude of the residuals in the original scale of the dependent variable:\n",
    "\n",
    "RMSE is often preferred when you want the evaluation metric to be in the same units as the target variable.\n",
    "\n",
    "3. Mean Absolute Error (MAE):\n",
    "MAE is a measure of the average absolute difference between predicted and actual values. It is calculated by taking the average of the absolute residuals:\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "MSE and RMSE: Both MSE and RMSE penalize larger errors more heavily than smaller errors due to the squaring operation. They are sensitive to outliers.\n",
    "MAE: MAE is less sensitive to outliers since it considers the absolute differences. It provides a measure of the average absolute size of errors.\n",
    "Choosing the Right Metric:\n",
    "\n",
    "MSE/RMSE: If large errors should be penalized more, use MSE or RMSE. They are suitable when the magnitude of errors matters.\n",
    "MAE: If you want a metric that is less sensitive to outliers and provides a more balanced view of errors, use MAE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2171e8b4-65d1-4cff-a05f-bca0998e6284",
   "metadata": {},
   "source": [
    "## 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a65263-63c0-4049-92e4-352a3eb7b67a",
   "metadata": {},
   "source": [
    "Advantages and Disadvantages of RMSE, MSE, and MAE in Regression Analysis:\n",
    "\n",
    "1. Mean Squared Error (MSE):\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Mathematically Convenient: Squaring the errors makes the metric mathematically convenient, facilitating mathematical analysis and optimization.\n",
    "Emphasis on Large Errors: MSE emphasizes larger errors due to the squaring operation, which might be desirable in some contexts.\n",
    "Disadvantages:\n",
    "\n",
    "Sensitive to Outliers: MSE is sensitive to outliers because it squares the errors. Outliers can have a disproportionately large impact on the metric.\n",
    "Units: The unit of MSE is the square of the unit of the dependent variable, making interpretation less intuitive.\n",
    "2. Root Mean Squared Error (RMSE):\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Same Scale as Target Variable: RMSE has the same scale as the target variable, making it more interpretable compared to MSE.\n",
    "Disadvantages:\n",
    "\n",
    "Sensitive to Outliers: Like MSE, RMSE is sensitive to outliers due to the squaring operation.\n",
    "Units: The unit of RMSE is the same as the unit of the dependent variable squared, making interpretation less intuitive.\n",
    "3. Mean Absolute Error (MAE):\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Robust to Outliers: MAE is less sensitive to outliers compared to MSE and RMSE because it uses absolute differences.\n",
    "Intuitive Interpretation: The unit of MAE is the same as the unit of the dependent variable, making it more intuitively interpretable.\n",
    "Disadvantages:\n",
    "\n",
    "Less Emphasis on Large Errors: MAE treats all errors equally, which means it may not emphasize larger errors as much as MSE and RMSE.\n",
    "Choosing the Right Metric:\n",
    "\n",
    "Task-Specific Considerations: The choice between MSE, RMSE, and MAE depends on the specific goals and characteristics of the regression task.\n",
    "Outliers: If the dataset contains outliers that need to be downplayed, MAE may be preferred. If outliers are crucial and need emphasis, MSE or RMSE may be more suitable.\n",
    "Interpretability: If having a metric with an intuitive interpretation in the same units as the dependent variable is essential, MAE or RMSE may be preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90399bfe-123a-4441-93bd-9ac804e47e91",
   "metadata": {},
   "source": [
    "## 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9eef9a9-9f2b-41e2-82b2-3f0c7cd4ca17",
   "metadata": {},
   "source": [
    "Lasso (Least Absolute Shrinkage and Selection Operator) regularization is a technique used in linear regression to prevent overfitting and feature selection by adding a penalty term to the cost function. The penalty term is the absolute value of the coefficients multiplied by a regularization parameter (lambda or alpha).\n",
    "\n",
    "\n",
    "Differences from Ridge Regularization:\n",
    "\n",
    "L1 vs. L2 Norm:\n",
    "\n",
    "Lasso uses the L1 norm penalty term, which is the sum of absolute values of coefficients. In contrast, Ridge regularization uses the L2 norm penalty term, which is the sum of squared values of coefficients.\n",
    "Feature Selection:\n",
    "\n",
    "Lasso has a built-in feature selection property. As the regularization parameter increases, some coefficients are driven to exactly zero, effectively removing corresponding features. Ridge tends to shrink coefficients towards zero but rarely sets them exactly to zero.\n",
    "Sparsity:\n",
    "\n",
    "Lasso tends to produce sparse models with fewer non-zero coefficients, leading to a simpler and more interpretable model. Ridge generally produces models with non-zero coefficients that are close to each other.\n",
    "When to Use Lasso Regularization:\n",
    "\n",
    "Lasso regularization is more appropriate in the following situations:\n",
    "\n",
    "Feature Selection:\n",
    "\n",
    "When there is a large number of features, and you suspect that many of them are irrelevant, Lasso can be effective in performing automatic feature selection by setting some coefficients to zero.\n",
    "Sparse Models:\n",
    "\n",
    "When a simpler, more interpretable model is desired, and sparsity in the coefficient vector is preferred.\n",
    "Dealing with Collinearity:\n",
    "\n",
    "Lasso can be useful in handling multicollinearity (high correlation between features) by selecting one feature from a group of highly correlated features and setting the coefficients of others to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3d0ef2-00d9-4c61-b8b2-bd407ae3199e",
   "metadata": {},
   "source": [
    "## 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394276ed-f7be-4eae-a2e1-fc952804254b",
   "metadata": {},
   "source": [
    "Regularized linear models help prevent overfitting in machine learning by adding a penalty term to the linear regression cost function. This penalty discourages overly complex models with excessively large coefficients, which are prone to fitting noise in the training data. Two common types of regularization are Lasso (L1 regularization) and Ridge (L2 regularization).\n",
    "\n",
    "1. Lasso Regularization (L1):\n",
    "\n",
    "In Lasso regularization, the penalty term is the absolute value of the coefficients multiplied by a regularization parameter (\n",
    "\n",
    "The L1 penalty encourages sparsity in the model by driving some coefficients to exactly zero. This property makes Lasso effective for feature selection, as irrelevant features may have their corresponding coefficients set to zero.\n",
    "Example:\n",
    "Suppose you have a dataset with 100 features, but only 10 features are truly relevant to the target variable. Without regularization, a linear model might assign non-zero coefficients to all 100 features, leading to overfitting. By applying Lasso regularization, some coefficients will be driven to zero, effectively selecting the 10 relevant features and preventing overfitting.\n",
    "\n",
    "2. Ridge Regularization (L2):\n",
    "\n",
    "In Ridge regularization, the penalty term is the squared value of the coefficients multiplied by a regularization parameter (\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "The L2 penalty discourages large coefficients but does not force them to be exactly zero. It helps to smooth the model by shrinking the magnitude of all coefficients, especially useful when dealing with multicollinearity.\n",
    "Example:\n",
    "Consider a scenario where two features are highly correlated. Without regularization, the model might assign large, opposite-signed coefficients to these correlated features, capturing noise. Ridge regularization helps to control the size of the coefficients, making the model more robust to multicollinearity and preventing overfitting.\n",
    "\n",
    "Benefits of Regularized Linear Models:\n",
    "\n",
    "Preventing Overfitting: Regularization prevents overfitting by penalizing overly complex models with large coefficients.\n",
    "Feature Selection: Lasso regularization facilitates automatic feature selection by driving some coefficients to zero.\n",
    "Handling Collinearity: Ridge regularization helps handle multicollinearity by smoothing the model and preventing extreme coefficient values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f29167-fa03-461b-a34f-5a2a54be7892",
   "metadata": {},
   "source": [
    "## 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196d1d5a-d660-4470-819e-1dd7d433ce87",
   "metadata": {},
   "source": [
    "\n",
    "Regularized linear models, such as Lasso (L1 regularization) and Ridge (L2 regularization), offer significant advantages in preventing overfitting and promoting model simplicity. However, they are not always the best choice for regression analysis, and there are limitations to consider:\n",
    "\n",
    "Loss of Interpretability:\n",
    "\n",
    "Regularization tends to shrink coefficients, and in some cases, set them exactly to zero. While this is beneficial for feature selection and model simplicity, it can result in a loss of interpretability. In situations where understanding the specific impact of each variable is crucial, regularized models may not be the best choice.\n",
    "Assumption of Linearity:\n",
    "\n",
    "Regularized linear models assume a linear relationship between the features and the target variable. If the true relationship is highly non-linear, these models may not capture complex patterns effectively. In such cases, non-linear models like decision trees or neural networks might be more suitable.\n",
    "Sensitivity to Hyperparameters:\n",
    "\n",
    "Regularized models have hyperparameters, such as the regularization parameter (\n",
    "�\n",
    "α). The performance of the model can be sensitive to the choice of these hyperparameters. Selecting the optimal values requires tuning, which may be computationally expensive and data-dependent.\n",
    "Not Suitable for All Datasets:\n",
    "\n",
    "Regularized models may not be the best choice for all types of datasets. In situations where the number of features is small, and overfitting is not a significant concern, simpler models like ordinary least squares (OLS) regression may provide better results without the need for regularization.\n",
    "Loss of Information:\n",
    "\n",
    "The regularization term adds a penalty to the cost function, influencing the model's behavior. While this helps prevent overfitting, it may also result in a loss of information, particularly when dealing with small datasets.\n",
    "Limited Handling of Outliers:\n",
    "\n",
    "Regularized models are sensitive to outliers, especially in the L2 regularization (Ridge) case. Outliers can disproportionately affect the penalty term, leading to biased coefficient estimates.\n",
    "Choice Between Lasso and Ridge:\n",
    "\n",
    "The choice between Lasso and Ridge regularization depends on the specific characteristics of the data. If sparsity and feature selection are essential, Lasso may be preferred. However, choosing between the two requires understanding the underlying data dynamics.\n",
    "Computational Complexity:\n",
    "\n",
    "Solving the optimization problems associated with regularization can be computationally expensive, particularly for large datasets. This complexity may limit the scalability of regularized models.\n",
    "When Regularized Linear Models May Not Be Ideal:\n",
    "\n",
    "Highly Non-linear Relationships: When the relationship between features and the target variable is highly non-linear, other models like decision trees, random forests, or neural networks might perform better.\n",
    "Interpretability Requirements: In situations where interpretability is a top priority, simpler linear models without regularization may be preferred.\n",
    "Small Datasets: In cases where the dataset is small, the risk of overfitting may be low, and regularized models may not provide significant advantages over simpler models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a46e03-1a49-43c1-bef6-e80aece7b23e",
   "metadata": {},
   "source": [
    "## 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392ce2aa-d45e-4716-bc23-bb7f89303711",
   "metadata": {},
   "source": [
    "Choosing between Model A and Model B based on RMSE (Root Mean Squared Error) and MAE (Mean Absolute Error) depends on the specific characteristics of the problem and the importance of different aspects of model performance.\n",
    "\n",
    "RMSE = 10 (Model A):\n",
    "\n",
    "RMSE penalizes larger errors more heavily due to the squaring operation.\n",
    "It is sensitive to outliers and tends to be influenced more by large errors.\n",
    "In situations where the impact of large errors is critical, RMSE might be a relevant metric.\n",
    "MAE = 8 (Model B):\n",
    "\n",
    "MAE treats all errors equally without emphasizing larger errors.\n",
    "It is less sensitive to outliers compared to RMSE.\n",
    "In scenarios where all errors, regardless of size, are considered equally important, MAE might be a suitable metric.\n",
    "Choosing Between RMSE and MAE:\n",
    "\n",
    "Importance of Large Errors:\n",
    "\n",
    "If large errors have a significant impact on the application (e.g., in financial modeling or safety-critical systems), RMSE might be a more appropriate metric as it penalizes these errors more.\n",
    "Robustness to Outliers:\n",
    "\n",
    "If the dataset contains outliers and you want the model to be less influenced by them, MAE could be a better choice as it treats all errors equally.\n",
    "Scale of the Dependent Variable:\n",
    "\n",
    "RMSE has the same unit as the dependent variable, while MAE has the same unit as the dependent variable but not squared. Consider the scale of the dependent variable when interpreting the magnitude of errors.\n",
    "Limitations and Considerations:\n",
    "\n",
    "Impact of Outliers: If Model A has a few large errors, it could significantly inflate the RMSE, making it appear worse than it might be overall. In such cases, Model B with a lower MAE might be more robust.\n",
    "\n",
    "Interpretability: The choice between RMSE and MAE also depends on the interpretability of the metric in the context of the problem. If stakeholders are more comfortable with interpreting errors on the same scale as the dependent variable, MAE might be preferred.\n",
    "\n",
    "Context-Specific Considerations: The choice between RMSE and MAE is often problem-specific. It's essential to consider the specific goals and requirements of the application, as well as the characteristics of the dataset.\n",
    "\n",
    "In summary, the decision between Model A and Model B depends on the specific goals of the analysis, the impact of outliers, and the importance of different error sizes. Both RMSE and MAE provide valuable information about model performance, and the choice between them should be guided by the specific context of the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb435530-edbc-477a-bb5f-7431ec4f4165",
   "metadata": {},
   "source": [
    "## 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6eec43-9fc0-4547-82d6-a0e4f94d1f65",
   "metadata": {},
   "source": [
    "Choosing between Ridge and Lasso regularization for Model A and Model B involves considering the characteristics of the problem and the trade-offs associated with each type of regularization. Here are some considerations:\n",
    "\n",
    "Model A: Ridge Regularization (\n",
    "�\n",
    "=\n",
    "0.1\n",
    "α=0.1):\n",
    "\n",
    "Ridge regularization adds a penalty term to the cost function based on the squared values of the coefficients.\n",
    "Ridge tends to shrink coefficients towards zero without forcing them to be exactly zero.\n",
    "It is effective in handling multicollinearity and preventing overfitting.\n",
    "Model B: Lasso Regularization (\n",
    "�\n",
    "=\n",
    "0.5\n",
    "α=0.5):\n",
    "\n",
    "Lasso regularization adds a penalty term based on the absolute values of the coefficients.\n",
    "Lasso has a built-in feature selection property as it tends to drive some coefficients exactly to zero.\n",
    "It is useful for sparse models and automatic feature selection.\n",
    "Choosing Between Ridge and Lasso:\n",
    "\n",
    "Feature Selection:\n",
    "\n",
    "If feature selection is a priority, and you want a model with fewer non-zero coefficients, Lasso (Model B) might be preferred. Ridge tends to shrink coefficients towards zero but rarely sets them exactly to zero.\n",
    "Handling Multicollinearity:\n",
    "\n",
    "If multicollinearity (high correlation between features) is a concern, Ridge (Model A) is generally more effective. Ridge regularization distributes the impact of correlated features more evenly, while Lasso might arbitrarily choose one feature over another.\n",
    "Interpretability:\n",
    "\n",
    "If interpretability is crucial, Ridge might be more suitable, as it does not force any coefficients to be exactly zero. Lasso, by setting some coefficients to zero, may make the model harder to interpret.\n",
    "Trade-Offs:\n",
    "\n",
    "Ridge and Lasso represent a trade-off between bias and variance. Ridge tends to provide a smoother model with less variance but might introduce more bias. Lasso, by driving some coefficients to zero, can reduce bias but might increase variance.\n",
    "Limitations and Considerations:\n",
    "\n",
    "Choice of Regularization Parameter (\n",
    "�\n",
    "α): The choice of the regularization parameter (\n",
    "�\n",
    "α) is critical. Different values of \n",
    "�\n",
    "α will result in different model behaviors. Proper hyperparameter tuning, such as cross-validation, is essential.\n",
    "\n",
    "Context-Specific Considerations: The choice between Ridge and Lasso depends on the specific context of the problem. If interpretability and handling multicollinearity are crucial, Ridge might be preferred. If sparsity and feature selection are more important, Lasso might be more appropriate.\n",
    "\n",
    "Ensemble Methods: In some cases, using a combination of Ridge and Lasso regularization, such as Elastic Net, may provide a balanced approach that incorporates both types of penalties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04274bc1-3150-4d67-bcc8-16d8e76b3125",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
